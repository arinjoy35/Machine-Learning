{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arinj\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook                    #importing all packages and libraries\n",
    "from keras.utils import np_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating numpy matrices after feature subtraction and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../GSC-Dataset/GSC-Features-Data/'\n",
    "gsc = pd.read_csv(path + 'GSC-Features.csv', sep=',', header=0)\n",
    "gsc.set_index('img_id', inplace = True)\n",
    "gsc_sp = pd.read_csv(path + 'same_pairs.csv', sep=',', header=0, usecols=[0,1])\n",
    "gsc_dfn = pd.read_csv(path + 'diffn_pairs.csv', sep=',', header=0, usecols=[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc_sp_concat = np.empty((0,1024), int)\n",
    "gsc_sp_diff = np.empty((0,512), int)\n",
    "gsc_dp_concat = np.empty((0,1024), int)\n",
    "gsc_dp_diff = np.empty((0,512), int)\n",
    "for i in range(1,5001):                                 #taking only 5000 data out of 790000\n",
    "    img_id_A = gsc_sp.loc[i][0]\n",
    "    img_id_B = gsc_sp.loc[i][1]\n",
    "    try:\n",
    "        concat = np.append(gsc.loc[img_id_A], gsc.loc[img_id_B])\n",
    "        diff = np.subtract(gsc.loc[img_id_A], gsc.loc[img_id_B])\n",
    "        gsc_sp_concat = np.append(gsc_sp_concat, [concat], axis=0)\n",
    "        gsc_sp_diff = np.append(gsc_sp_diff, [diff], axis=0)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "np.savetxt('same_pair_concat_pickle.csv', gsc_sp_concat, delimiter=',')\n",
    "np.savetxt('same_pair_diff_pickle.csv', gsc_sp_diff, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5001):\n",
    "    img_id_A = gsc_dfn.loc[i][0]\n",
    "    img_id_B = gsc_dfn.loc[i][1]\n",
    "    try:\n",
    "        concat = np.append(gsc.loc[img_id_A], gsc.loc[img_id_B])\n",
    "        diff = np.subtract(gsc.loc[img_id_A], gsc.loc[img_id_B])\n",
    "        gsc_dp_concat = np.append(gsc_dp_concat, [concat], axis=0)\n",
    "        gsc_dp_diff = np.append(gsc_dp_diff, [diff], axis=0)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "np.savetxt('diff_pair_concat_pickle.csv', gsc_sp_concat, delimiter=',')\n",
    "np.savetxt('diff_pair_diff_pickle.csv', gsc_sp_diff, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"same_pair_concat_pickle.csv\" , header=None)\n",
    "df['1024'] = '1'\n",
    "df.to_csv('same_pair_concat_pickle_with_ones.csv', header=False, index=False)\n",
    "\n",
    "df = pd.read_csv(\"same_pair_diff_pickle.csv\" , header=None)\n",
    "df['512'] = '1'\n",
    "df.to_csv('same_pair_diff_pickle_with_ones.csv', header=False, index=False)\n",
    "\n",
    "df = pd.read_csv(\"diff_pair_concat_pickle.csv\" , header=None)\n",
    "df['1024'] = '0'\n",
    "df.to_csv('diff_pair_concat_pickle_with_zeroes.csv', header=False, index=False)\n",
    "\n",
    "df = pd.read_csv(\"diff_pair_diff_pickle.csv\" , header=None)\n",
    "df['512'] = '0'\n",
    "df.to_csv('diff_pair_diff_pickle_with_zeroes.csv', header=False, index=False)\n",
    "\n",
    "dp_concat_temp = pd.read_csv('diff_pair_concat_pickle_with_zeroes.csv', sep=',')\n",
    "dp_diff_temp = pd.read_csv('diff_pair_diff_pickle_with_zeroes.csv', sep=',')\n",
    "sp_concat_temp = pd.read_csv('same_pair_concat_pickle_with_ones.csv', sep=',')\n",
    "sp_diff_temp = pd.read_csv('same_pair_diff_pickle_with_ones.csv', sep=',')\n",
    "\n",
    "a = np.array(dp_concat_temp)\n",
    "b = np.array(dp_diff_temp)\n",
    "c = np.array(sp_concat_temp[:2001])\n",
    "d = np.array(sp_diff_temp[:2001])\n",
    "\n",
    "concat_matrix=np.concatenate((c,a),axis=0)\n",
    "np.random.shuffle(concat_matrix)\n",
    "diff_matrix=np.concatenate((d,b),axis=0)\n",
    "np.random.shuffle(diff_matrix)\n",
    "\n",
    "np.savetxt('concat_matrix_pickle.csv', concat_matrix, delimiter=',')\n",
    "np.savetxt('diff_matrix_pickle.csv', diff_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning data into training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_con = pd.read_csv('concat_matrix_pickle.csv', sep=',', header=None, usecols=[1024])\n",
    "target_vector_concat=np.array(target_con)\n",
    "target_dif = pd.read_csv('diff_matrix_pickle.csv', sep=',', header=None, usecols=[512])\n",
    "target_vector_diff=np.array(target_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingData(given_matrix,given_target):\n",
    "    n, m = given_matrix.shape\n",
    "    n_train = int(0.9 * n)\n",
    "    n_valid = int((n - n_train) / 2)\n",
    "\n",
    "    x_train = given_matrix.loc[:n_train, :]\n",
    "    return x_train\n",
    "\n",
    "def ValidationData(given_matrix,given_target):\n",
    "    n, m = given_matrix.shape\n",
    "    n_train = int(0.9 * n)\n",
    "    n_valid = int((n - n_train) / 2)\n",
    "\n",
    "    x_valid = given_matrix.loc[n_train:n_train + n_valid, :]\n",
    "    return x_valid\n",
    "\n",
    "def TestingData(given_matrix,given_target):\n",
    "    n, m = given_matrix.shape\n",
    "    n_train = int(0.9 * n)\n",
    "    n_valid = int((n - n_train) / 2)\n",
    "\n",
    "    x_test = given_matrix.loc[n_train + n_valid:, :]\n",
    "    return x_test\n",
    "\n",
    "\n",
    "def TrainingVector(given_matrix,given_target):\n",
    "    n, m = given_matrix.shape\n",
    "    n_train = int(0.9 * n)\n",
    "    n_valid = int((n - n_train) / 2)\n",
    "\n",
    "    y_train = given_target[:n_train, :]\n",
    "    return y_train\n",
    "\n",
    "\n",
    "def ValidationVector(given_matrix,given_target):\n",
    "    n, m = given_matrix.shape\n",
    "    n_train = int(0.9 * n)\n",
    "    n_valid = int((n - n_train) / 2)\n",
    "\n",
    "    y_valid = given_target[n_train:n_train + n_valid, :]\n",
    "    return y_valid\n",
    "\n",
    "\n",
    "def TestingVector(given_matrix,given_target):\n",
    "    n, m = given_matrix.shape\n",
    "    n_train = int(0.9 * n)\n",
    "    n_valid = int((n - n_train) / 2)\n",
    "\n",
    "    y_test = given_target[n_train + n_valid:, :]\n",
    "    return y_test\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = pd.read_csv('concat_matrix_pickle.csv', sep=',', header=None, usecols=range(1024))\n",
    "diff_data = pd.read_csv('diff_matrix_pickle.csv', sep=',', header=None, usecols=range(512))\n",
    "diff_data = diff_data.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### Concatenation ##########################\n",
      "(6301, 1024)\n",
      "(351, 1024)\n",
      "(350, 1024)\n",
      "(6300, 1)\n",
      "(350, 1)\n",
      "(350, 1)\n",
      "###################### Subtraction ############################\n",
      "(6301, 512)\n",
      "(351, 512)\n",
      "(350, 512)\n",
      "(6300, 1)\n",
      "(350, 1)\n",
      "(350, 1)\n"
     ]
    }
   ],
   "source": [
    "print('###################### Concatenation ##########################')\n",
    "\n",
    "concat_train_data = TrainingData(concat_data,target_vector_concat) \n",
    "print(concat_train_data.shape)\n",
    "concat_valid_data = ValidationData(concat_data,target_vector_concat) \n",
    "print(concat_valid_data.shape)\n",
    "concat_test_data = TestingData(concat_data,target_vector_concat) \n",
    "print(concat_test_data.shape)\n",
    "concat_train_vector = TrainingVector(concat_data,target_vector_concat) \n",
    "print(concat_train_vector.shape)\n",
    "concat_valid_vector = ValidationVector(concat_data,target_vector_concat) \n",
    "print(concat_valid_vector.shape)\n",
    "concat_test_vector = TestingVector(concat_data,target_vector_concat) \n",
    "print(concat_test_vector.shape)\n",
    "\n",
    "print('###################### Subtraction ############################')\n",
    "\n",
    "diff_train_data = TrainingData(diff_data,target_vector_diff) \n",
    "print(diff_train_data.shape)\n",
    "diff_valid_data = ValidationData(diff_data,target_vector_diff) \n",
    "print(diff_valid_data.shape)\n",
    "diff_test_data = TestingData(diff_data,target_vector_diff) \n",
    "print(diff_test_data.shape)\n",
    "diff_train_vector = TrainingVector(diff_data,target_vector_diff) \n",
    "print(diff_train_vector.shape)\n",
    "diff_valid_vector = ValidationVector(diff_data,target_vector_diff) \n",
    "print(diff_valid_vector.shape)\n",
    "diff_test_vector = TestingVector(diff_data,target_vector_diff) \n",
    "print(diff_test_vector.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAcc = 0.0\n",
    "maxIter = 0\n",
    "TrainingPercent = 90\n",
    "ValidationPercent = 5\n",
    "TestPercent = 5\n",
    "M = 100\n",
    "PHI = []\n",
    "IsSynthetic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,(len(VAL_TEST_OUT)-1)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "\n",
    "def GetAccuracy(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,(len(VAL_TEST_OUT)-1)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    return (str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1024)\n",
      "(1024, 1024)\n",
      "(6300, 100)\n",
      "(100,)\n",
      "(351, 100)\n",
      "(350, 100)\n"
     ]
    }
   ],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(concat_train_data)\n",
    "Mu = kmeans.cluster_centers_\n",
    "\n",
    "concat_data_T = np.array(np.transpose(concat_data))\n",
    "concat_test_data_T = np.array(np.transpose(concat_test_data))\n",
    "concat_valid_data_T = np.array(np.transpose(concat_valid_data))\n",
    "\n",
    "BigSigma1     = GenerateBigSigma(concat_data_T, Mu, TrainingPercent,IsSynthetic)\n",
    "\n",
    "BigSigma =    np.identity(len(BigSigma1))*0.2 + BigSigma1\n",
    "\n",
    "TRAINING_PHI = GetPhiMatrix(concat_data_T, Mu, BigSigma, TrainingPercent)\n",
    "TEST_PHI     = GetPhiMatrix(concat_test_data_T, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(concat_valid_data_T, Mu, BigSigma, 100)\n",
    "W            = np.random.rand(M,)\n",
    "\n",
    "print(Mu.shape)\n",
    "print(BigSigma.shape)\n",
    "print(TRAINING_PHI.shape)\n",
    "print(W.shape)\n",
    "print(VAL_PHI.shape)\n",
    "print(TEST_PHI.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Now        = np.dot(220, W)\n",
    "La           = 2\n",
    "learningRate = 0.01\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []\n",
    "W_Mat        = []\n",
    "TrainingTarget = (np.transpose(concat_train_vector)).flatten()\n",
    "ValDataAct = np.array(concat_valid_vector)\n",
    "TestDataAct = np.array(concat_test_vector)\n",
    "\n",
    "for i in range(0,1000):\n",
    "    #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "    Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "    \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "    Acc_TR        = GetAccuracy(TR_TEST_OUT,TrainingTarget)\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "    Acc_Val       = GetAccuracy(VAL_TEST_OUT,ValDataAct)\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "    Erms_Test     = GetErms(TEST_OUT,TestDataAct)\n",
    "    Acc_Test      = GetAccuracy(TEST_OUT,TestDataAct)\n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------LINEAR REGRESSION------------------------------------\n",
      "----------Gradient Descent Solution for Concatenation--------------------\n",
      "M = 100 \n",
      "Lambda  = 2\n",
      "eta = 0.01\n",
      "E_rms Training   = 0.44302122\n",
      "E_rms Validation = 0.43192941\n",
      "E_rms Testing    = 0.46094831\n",
      "Training Accuracy = 71.46031746031746%\n",
      "Validation Accuracy = 72.07977207977208%\n",
      "Testing Accuracy = 69.14285714285714%\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('--------------------LINEAR REGRESSION------------------------------------')\n",
    "print ('----------Gradient Descent Solution for Concatenation--------------------')\n",
    "print (\"M = 100 \\nLambda  = 2\\neta = 0.01\")\n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),8)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),8)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),8)))\n",
    "print (\"Training Accuracy = \" + Acc_TR + \"%\")\n",
    "print (\"Validation Accuracy = \" + Acc_Val + \"%\")\n",
    "print (\"Testing Accuracy = \" + Acc_Test + \"%\")\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 512)\n",
      "(512, 512)\n",
      "(6300, 100)\n",
      "(100,)\n",
      "(351, 100)\n",
      "(350, 100)\n"
     ]
    }
   ],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(diff_train_data)\n",
    "Mu = kmeans.cluster_centers_\n",
    "\n",
    "diff_data_T = np.array(np.transpose(diff_data))\n",
    "diff_test_data_T = np.array(np.transpose(diff_test_data))\n",
    "diff_valid_data_T = np.array(np.transpose(diff_valid_data))\n",
    "\n",
    "BigSigma1     = GenerateBigSigma(diff_data_T, Mu, TrainingPercent,IsSynthetic)\n",
    "\n",
    "BigSigma =    np.identity(len(BigSigma1))*0.2 + BigSigma1\n",
    "\n",
    "TRAINING_PHI = GetPhiMatrix(diff_data_T, Mu, BigSigma, TrainingPercent)\n",
    "TEST_PHI     = GetPhiMatrix(diff_test_data_T, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(diff_valid_data_T, Mu, BigSigma, 100)\n",
    "W            = np.random.rand(M,)\n",
    "\n",
    "print(Mu.shape)\n",
    "print(BigSigma.shape)\n",
    "print(TRAINING_PHI.shape)\n",
    "print(W.shape)\n",
    "print(VAL_PHI.shape)\n",
    "print(TEST_PHI.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Now        = np.dot(220, W)\n",
    "La           = 2\n",
    "learningRate = 0.01\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []\n",
    "W_Mat        = []\n",
    "TrainingTarget = (np.transpose(diff_train_vector)).flatten()\n",
    "ValDataAct = np.array(diff_valid_vector)\n",
    "TestDataAct = np.array(diff_test_vector)\n",
    "\n",
    "for i in range(0,1000):\n",
    "    #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "    Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "    \n",
    "     #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "    Acc_TR        = GetAccuracy(TR_TEST_OUT,TrainingTarget)\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "    Acc_Val       = GetAccuracy(VAL_TEST_OUT,ValDataAct)\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "    Erms_Test     = GetErms(TEST_OUT,TestDataAct)\n",
    "    Acc_Test      = GetAccuracy(TEST_OUT,TestDataAct)\n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------LINEAR REGRESSION----------------------------------\n",
      "----------Gradient Descent Solution for Subtraction--------------------\n",
      "M = 100 \n",
      "Lambda  = 2\n",
      "eta = 0.01\n",
      "E_rms Training   = 0.44675744\n",
      "E_rms Validation = 0.44174172\n",
      "E_rms Testing    = 0.43513031\n",
      "Training Accuracy = 71.2063492063492%\n",
      "Validation Accuracy = 72.07977207977208%\n",
      "Testing Accuracy = 73.71428571428571%\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('--------------------LINEAR REGRESSION----------------------------------')\n",
    "print ('----------Gradient Descent Solution for Subtraction--------------------')\n",
    "print (\"M = 100 \\nLambda  = 2\\neta = 0.01\")\n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),8)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),8)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),8)))\n",
    "print (\"Training Accuracy = \" + Acc_TR + \"%\")\n",
    "print (\"Validation Accuracy = \" + Acc_Val + \"%\")\n",
    "print (\"Testing Accuracy = \" + Acc_Test + \"%\")\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(parameter):\n",
    "    return (1/(1+(np.exp(-1*parameter))))\n",
    "\n",
    "def predict(features, weights):\n",
    "    z = np.dot(features, weights)\n",
    "    answer = sigmoid(z)\n",
    "    return answer\n",
    "\n",
    "def cost_function(features, labels, weights):\n",
    "    observations = len(labels)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "    \n",
    "    #Take the error when label=1\n",
    "    class1_cost = -labels*np.log(predictions)\n",
    "    \n",
    "    #Take the error when label=0\n",
    "    class2_cost = (1-labels)*np.log(1-predictions)\n",
    "    \n",
    "    #Take the sum of both costs\n",
    "    cost = class1_cost - class2_cost\n",
    "\n",
    "    #Take the average cost\n",
    "    cost = cost.sum()/observations\n",
    "\n",
    "    return cost\n",
    "\n",
    "def update_weights(features, labels, weights, lr):\n",
    "    \n",
    "    N = len(features)\n",
    "\n",
    "    #1 - Get Predictions\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #2 Transpose features \n",
    "    gradient = np.dot(features.T,  predictions - labels)\n",
    "\n",
    "    #3 Take the average cost derivative for each feature\n",
    "    gradient /= N\n",
    "\n",
    "    #4 - Multiply the gradient by our learning rate\n",
    "    gradient *= lr\n",
    "\n",
    "    #5 - Subtract from our weights to minimize cost\n",
    "    weights -= gradient\n",
    "\n",
    "    return weights\n",
    "\n",
    "def train_logistic_weight(features, labels, weights, lr, iters):\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr)\n",
    "\n",
    "        #Calculate error for auditing purposes\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return weights\n",
    "\n",
    "def train_logistic_cost(features, labels, weights, lr, iters):\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr)\n",
    "\n",
    "        #Calculate error for auditing purposes\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return cost_history\n",
    "\n",
    "def accuracy_Logistic(predicted_labels, actual_labels):\n",
    "    count = 0\n",
    "    a = actual_labels\n",
    "    b = np.array(predicted_labels)\n",
    "    c = np.absolute(b)\n",
    "    d = np.around(c)\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        if (a[i]  == d[i]):\n",
    "            count += 1\n",
    "    \n",
    "    return (1-(count/len(a)))*100\n",
    "\n",
    "def Erms_Logistic(VAL_TEST_OUT,cost_last):\n",
    "    return (math.sqrt(2*cost_last/len(VAL_TEST_OUT)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Logistic        = np.dot(220, W_Now)\n",
    "concat_data_T = np.array(np.transpose(concat_data))\n",
    "concat_test_data_T = np.array(np.transpose(concat_test_data))\n",
    "concat_valid_data_T = np.array(np.transpose(concat_valid_data))\n",
    "\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(concat_train_data)\n",
    "Mu = kmeans.cluster_centers_\n",
    "\n",
    "BigSigma1     = GenerateBigSigma(concat_data_T, Mu, TrainingPercent,IsSynthetic)\n",
    "\n",
    "BigSigma =    np.identity(len(BigSigma1))*0.2 + BigSigma1\n",
    "\n",
    "TRAINING_PHI = GetPhiMatrix(concat_data_T, Mu, BigSigma, TrainingPercent)\n",
    "TEST_PHI     = GetPhiMatrix(concat_test_data_T, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(concat_valid_data_T, Mu, BigSigma, 100)\n",
    "\n",
    "TrainingTarget = (np.transpose(concat_train_vector)).flatten()\n",
    "ValDataAct = np.array(concat_valid_vector)\n",
    "TestDataAct = np.array(concat_test_vector)\n",
    "\n",
    "current_w = train_logistic_weight(TRAINING_PHI,TrainingTarget,W_Logistic,0.01,2000)\n",
    "current_cost_hist = train_logistic_cost(TRAINING_PHI,TrainingTarget,W_Logistic,0.01,2000)\n",
    "\n",
    "#-----------------TrainingData---------------------#\n",
    "TR_TEST_OUT   = GetValTest(TRAINING_PHI,current_w) \n",
    "Erms_TR       = Erms_Logistic(TR_TEST_OUT,current_cost_hist[-1])\n",
    "Acc_TR        = accuracy_Logistic(TR_TEST_OUT,TrainingTarget)\n",
    "        \n",
    "#-----------------ValidationData---------------------#\n",
    "VAL_TEST_OUT  = GetValTest(VAL_PHI,current_w) \n",
    "Erms_Val      = Erms_Logistic(VAL_TEST_OUT,current_cost_hist[-1])\n",
    "Acc_Val       = accuracy_Logistic(VAL_TEST_OUT,ValDataAct)\n",
    "    \n",
    "#-----------------TestingData---------------------#\n",
    "TEST_OUT      = GetValTest(TEST_PHI,current_w) \n",
    "Erms_Test     = Erms_Logistic(TEST_OUT,current_cost_hist[-1])\n",
    "Acc_Test      = accuracy_Logistic(TEST_OUT,TestDataAct)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------LOGISTIC REGRESSION------------------------------------\n",
      "----------Gradient Descent Solution for Concatenation--------------------\n",
      "M = 100 \n",
      "Iteration  = 2000\n",
      "eta = 0.01\n",
      "E_rms Training   = 0.015211089975801267\n",
      "E_rms Validation = 0.06444319318680147\n",
      "E_rms Testing    = 0.06453518922676675\n",
      "Training Accuracy = 37.25396825396825%\n",
      "Validation Accuracy = 31.14285714285714%\n",
      "Testing Accuracy = 39.142857142857146%\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('--------------------LOGISTIC REGRESSION------------------------------------')\n",
    "print ('----------Gradient Descent Solution for Concatenation--------------------')\n",
    "print (\"M = 100 \\nIteration  = 2000\\neta = 0.01\")\n",
    "print (\"E_rms Training   = \" + str(Erms_TR))\n",
    "print (\"E_rms Validation = \" + str(Erms_Val))\n",
    "print (\"E_rms Testing    = \" + str(Erms_Test))\n",
    "print (\"Training Accuracy = \" + str(Acc_TR) + \"%\")\n",
    "print (\"Validation Accuracy = \" + str(Acc_Val) + \"%\")\n",
    "print (\"Testing Accuracy = \" + str(Acc_Test) + \"%\")\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arinj\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\arinj\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "W_Logistic        = np.dot(220, W_Now)\n",
    "diff_data_T = np.array(np.transpose(diff_data))\n",
    "diff_test_data_T = np.array(np.transpose(diff_test_data))\n",
    "diff_valid_data_T = np.array(np.transpose(diff_valid_data))\n",
    "\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(diff_train_data)\n",
    "Mu = kmeans.cluster_centers_\n",
    "\n",
    "BigSigma1     = GenerateBigSigma(diff_data_T, Mu, TrainingPercent,IsSynthetic)\n",
    "\n",
    "BigSigma =    np.identity(len(BigSigma1))*0.2 + BigSigma1\n",
    "\n",
    "TRAINING_PHI = GetPhiMatrix(diff_data_T, Mu, BigSigma, TrainingPercent)\n",
    "TEST_PHI     = GetPhiMatrix(diff_test_data_T, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(diff_valid_data_T, Mu, BigSigma, 100)\n",
    "\n",
    "TrainingTarget = (np.transpose(diff_train_vector)).flatten()\n",
    "ValDataAct = np.array(diff_valid_vector)\n",
    "TestDataAct = np.array(diff_test_vector)\n",
    "\n",
    "current_w = train_logistic_weight(TRAINING_PHI,TrainingTarget,W_Logistic,0.01,2000)\n",
    "current_cost_hist = train_logistic_cost(TRAINING_PHI,TrainingTarget,W_Logistic,0.01,2000)\n",
    "\n",
    "#-----------------TrainingData---------------------#\n",
    "TR_TEST_OUT   = GetValTest(TRAINING_PHI,current_w) \n",
    "Erms_TR       = Erms_Logistic(TR_TEST_OUT,current_cost_hist[-1])\n",
    "Acc_TR        = accuracy_Logistic(TR_TEST_OUT,TrainingTarget)\n",
    "        \n",
    "#-----------------ValidationData---------------------#\n",
    "VAL_TEST_OUT  = GetValTest(VAL_PHI,current_w) \n",
    "Erms_Val      = Erms_Logistic(VAL_TEST_OUT,current_cost_hist[-1])\n",
    "Acc_Val       = accuracy_Logistic(VAL_TEST_OUT,ValDataAct)\n",
    "    \n",
    "#-----------------TestingData---------------------#\n",
    "TEST_OUT      = GetValTest(TEST_PHI,current_w) \n",
    "Erms_Test     = Erms_Logistic(TEST_OUT,current_cost_hist[-1])\n",
    "Acc_Test      = accuracy_Logistic(TEST_OUT,TestDataAct)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------LOGISTIC REGRESSION------------------------------------\n",
      "----------Gradient Descent Solution for Subtraction--------------------\n",
      "M = 100 \n",
      "Iteration  = 2000\n",
      "eta = 0.01\n",
      "E_rms Training   = 0.014028957762318956\n",
      "E_rms Validation = 0.059434980446822015\n",
      "E_rms Testing    = 0.05951982700029232\n",
      "Training Accuracy = 61.92063492063492%\n",
      "Validation Accuracy = 63.142857142857146%\n",
      "Testing Accuracy = 66.57142857142857%\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('--------------------LOGISTIC REGRESSION------------------------------------')\n",
    "print ('----------Gradient Descent Solution for Subtraction--------------------')\n",
    "print (\"M = 100 \\nIteration  = 2000\\neta = 0.01\")\n",
    "print (\"E_rms Training   = \" + str(Erms_TR))\n",
    "print (\"E_rms Validation = \" + str(Erms_Val))\n",
    "print (\"E_rms Testing    = \" + str(Erms_Test))\n",
    "print (\"Training Accuracy = \" + str(Acc_TR) + \"%\")\n",
    "print (\"Validation Accuracy = \" + str(Acc_Val) + \"%\")\n",
    "print (\"Testing Accuracy = \" + str(Acc_Test) + \"%\")\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Placeholder\n",
    "inputTensor  = tf.placeholder(tf.float32, [None, 1024])\n",
    "outputTensor = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_NEURONS_LAYER_1 = 100\n",
    "NUM_HIDDEN_NEURONS_LAYER_2 = 100\n",
    "\n",
    "LEARNING_RATE = 1\n",
    "\n",
    "# Initializing the weights to Normal Distribution\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "\n",
    "#2 hidden layer\n",
    "input_hidden_weights  = init_weights([1024, NUM_HIDDEN_NEURONS_LAYER_1])\n",
    "\n",
    "#initializing the hidden layer 1 to output layer weights 1\n",
    "hidden_output_weights1 = init_weights([NUM_HIDDEN_NEURONS_LAYER_1, NUM_HIDDEN_NEURONS_LAYER_2])\n",
    "\n",
    "#initializing the input to hidden layer 2 weights\n",
    "input_hidden_weights2 = init_weights([NUM_HIDDEN_NEURONS_LAYER_1, NUM_HIDDEN_NEURONS_LAYER_2])\n",
    "\n",
    "#initializing the output to hidden layer 2 weights\n",
    "hidden_output_weights2 = init_weights([NUM_HIDDEN_NEURONS_LAYER_2, 1])\n",
    "\n",
    "# Computing values at the hidden layer 1\n",
    "hidden_layer1 = tf.nn.relu(tf.matmul(inputTensor, input_hidden_weights))\n",
    "\n",
    "#computing values at the hidden layer 2\n",
    "hidden_layer2 = tf.nn.relu(tf.matmul(hidden_layer1, input_hidden_weights2))\n",
    "\n",
    "# Computing values at the output layer\n",
    "output_layer = tf.matmul(hidden_layer2, hidden_output_weights2)\n",
    "\n",
    "# Defining Error Function\n",
    "error_function = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_layer, labels=outputTensor))\n",
    "\n",
    "# Defining Learning Algorithm and Training Parameters\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "\n",
    "# Prediction Function\n",
    "prediction = tf.round(tf.sigmoid(output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dff54f6df5049dea0ae8202eda35665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_EPOCHS = 5000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "processedTrainingData = np.array(concat_train_data)\n",
    "processedTrainingLabel = np.array(concat_train_vector)\n",
    "processedTestingData = np.array(concat_test_data)\n",
    "processedTestingLabel = np.array(concat_test_vector)\n",
    "\n",
    "training_accuracy = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Set Global Variables ?\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(NUM_OF_EPOCHS)):\n",
    "        \n",
    "        #Shuffle the Training Dataset at each epoch\n",
    "        p = np.array(np.random.permutation(range(len(processedTrainingData)-1)),dtype='int')\n",
    "        processedTrainingData  = processedTrainingData[p]\n",
    "        processedTrainingLabel = processedTrainingLabel[p]\n",
    "        \n",
    "        # Start batch training\n",
    "        for start in range(0, len(processedTrainingData), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(training, feed_dict={inputTensor: processedTrainingData[start:end], \n",
    "                                          outputTensor: processedTrainingLabel[start:end]})\n",
    "        # Training accuracy for an epoch\n",
    "        training_accuracy.append(np.mean(np.argmax(processedTrainingLabel, axis=1) ==\n",
    "                             sess.run(prediction, feed_dict={inputTensor: processedTrainingData,\n",
    "                                                             outputTensor: processedTrainingLabel})))\n",
    "    # Testing\n",
    "    predictedTestLabel = sess.run(prediction, feed_dict={inputTensor: processedTestingData})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x248a0d29668>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHOFJREFUeJzt3Xt4XPV95/H3V6ObZcvyXdiWQTI2xffYiFsgIMAQmyYmT8om0CUFQvHTbUiaJaVrmoWE7G6zpU2bh0IC2qck2X2SEKeUxgVjJ4AH2gbM3RhfhGXZ2IoNMvJVxpIlzW//mDPyaDw3STMenaPP63n0zLn8zpnfb+boo59+c84Zc84hIiLBUlToCoiISO4p3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJIAU7iIiAaRwFxEJIIW7iEgAFRfqiSdNmuRqa2sHte3x48cZPXp0bis0zKnNI4PaPDIMpc1vvPHGR865yZnKFSzca2tref311we1bTgcpqGhIbcVGubU5pFBbR4ZhtJmM3s/m3IalhERCSCFu4hIACncRUQCSOEuIhJACncRkQDKGO5m9riZtZnZuynWm5k9ZGbNZvaOmS3JfTVFRGQgsum5/xhYlmb9cmC297MS+OHQqyUiIkORMdydcy8BB9MUuQH4vy7qFWCcmU3NVQUTvb33MH/96gm27DuSr6cQEfG9XFzENB3YGzff6i3bn1jQzFYS7d1TXV1NOBwe8JOt3XWSbQcj/P5D/84d80v5VE3JoCoNsONQLwc7HRdPzfwyHPg4wo7DET45rTDXfXV0dAzq9RrunHMcPQntnRFCBsVmlBXD6BLjeMdxnn1uA7Gv+U38tt/Er//t7HX0RCDioNdBxDk6e6LzybZP3Ef/9a5vfWdvdIkBZsmnAYpOW2dJt4mV7zftzZ84cYKWXz0fnY8rF91/dKI74igtMqrKjM0f9fJx9+ktS9bW7l5HV2+SFSn0OujqjT7X8W7H6BJjemURnT2OshBMGlXEwU5HaQjaTzje+aiXslD6fSb7yuaJpT38dNt6KkuNk72wYFKIiIOSEGw60Ms5lUWMKY223RK2TXwPADq6o8cBJHtPkrwPnHq/4ssfO+lo73T0RBxm0X19dMLx0QnHwRMRPntuKbVjiygJRdcdOwntJyKc7I3Ox/909kTrFZufVtoFef59zkVSJb7ekPzYwjnXCDQC1NfXu8FcobW/Yg+rmzYD8I/vnuS+W67Narvu3gg/+o9dHD3Rw58tnU1JqIjbVj0DwH+7eWnG7S/97vPsP9LFzFnnsan1MD9/dW/GbZKZPWUMHx7t5GhnDwDFRcasKWP6lXEOmj48BsCimioccPRoiN1Hj/eVWTRjHEUGb+05DMAF54znjfcPATBn6liKi4yIc0RcNEB7Io7mtg7GlBUzfdyo6MFt0fApKorGUJF3lBd5B36RWV855xxtx7rojZx6ay3unTfvMOi/LG46bkVs6mRvhPaOk5zoTpU4Bnyc5tUMIgM6C12JPqNLQ5zs7aW7N+mv9GmmVZVTWX6qw2WJ6ZAwv/2DY97Cnr5lT7d0D66yZ9gPN3UNettb55bxx3m+KjcX4d4KzIibrwH25WC/OfV//q2FB9c1AXD2hAq+cOGMtOUffmEHf/vr99jxv5ZTEirio47oG7nqnzdnfK7iIqMnkvyXYXb1GHa0dfTNLzl7PONH9//vo7M7QtOHx5hWVc64ilLM4MChU/urmzSaseX937od3h8DgOqxZRRZrBcSfdx35AQAHV091E6q8EI/GvyO6GPE0TftvJ6vc+BwRDBmTxnD2FFeXZP0eF1ctyy+9al6xyGD0WXFOOCcCRWcM3E0x7t6aDvWxYnuXj5sfZ9Z557b/w+G9e/BxffGRpWGKCsOESoyiouMUJExqjRESejU6KMl7Cd+H/Hr45eXl0T3GYlEX4u+/yTcqfn415B+y+Jf44TphG0dsGnTOyxYsOC05c7bb8TBD8LNlIaKeNP7w37H5XV85apZJErM1VDIGFs+uP9024510tHZgwO27z9GTyTCL17by293tnPbJ2v5o0vPYUxZMVPGlg9ov53dvax74UUmzlzAh0e7+OXre7n6/Ck0fXiMhdOr+N5v3uPrS89jztTKvoPn1PEWm3f95suKi6iqKPFe7+hxHFsf/5rHju/E9yu2vLTYmDGhgvEVpfRGHBHn2H+kk3XvfsArLe1MGF3K8a4erplTTW/EESoyzqoqp7qyHDPojTh6I9GOlXOOqeNGETKjJxJh8xsbB/U+DEQuwn0NcJeZPQFcDBxxzp02JFNoR06c6g38xZPv8N1nt/Vb/377ca753ous+/qnON7Vy9/++r2+7SaNKUvZczl7QgXfv+kTLDk72nPu6unlk+dOYueBDh7416289N4BtjzwaXa3H2dcRSnTx42is7uX9Vs+YMWiaf1CJp1wOEznpPO54rxJVJSeett6Iw4DPuro4tq/f4mf/vHFzJ9edfr2TW3c9qPXuPK8yTz2pfqsnrPQwuF9NFwxs9DVOKNsfzENc6rTlrl+QfQjrQfXbecH4Z1MGF3KhNGlea3XlMpyplRGp8+dHP1Pc3xFKb/d2c4X6mcwc/KYNFunVl4SYlxZEZ+aHb0P1o0X1PRbf9tldYOvdA6VeMNN504ew1eumpX0j+lANBdn93s/FBnD3cx+DjQAk8ysFfgWUALgnHsUWAtcDzQT/R/69nxVNpnaVc+w67vXZwzJooT1hz4+FfYvbP+Qu1dvoifiWPp3L/UrV/8/n0u5z1XLz+eWS6I9FogOjcScO3kMjV+6gMMfdzO6rJh5004FbnlJiBs+MT1z4xIsm3/WactC3iDhlLHlbPrWdSm3jQ2nxMqLDNUV502m5a+up0jH1LCUMdydczdnWO+Ar+SsRoNQd+9aHv7DxXxm4bSUZfa0px67/fKPB353yj+58lz+5Mpz05YpLwlxVlWGT5jOkNgwUeIfOZGhULAPX4G5QvWun73FN1Zv6jfuG/OzjXt4ZnNuR4q+vnR2TveXbzMnRe8dfdX5GW8DLSIB4Ptwf/qrl/dNP/lmK3/ww9/2O6MD4C+fyvwh6ECVlwyPHnm2ZldX8uZ91/KHF51d6KqIyBng+3CfP72KDX/e0Df/5p7DXPHghrw+51N/+sm87j9fJowuzfoDXBHxN9+HO0RPDXz3gU/3zf/u8Am+8NjLvPu7I9R657Ln0uKzx2cuJJJn2Z15LiNVIMId6DtjJebVXQf5zD/8e4FqIyJSWL4O90dvuaDf/AMr5g15n7dees6Q9yFyJmiATdLxdbjX1/YfHpk5eWjfoP6Na887bdkzX7s8SUkRkeHNd+Ee31uZUNH/qrxPzZ7MtKqBXf4c73OLT7+waM5ZY/vNj/LZWTIiMjL5LtzjJbuAYlZ15WnL5k8fe9qybJnBS/dcxdeuiZ7X/uXLawe9LxGRM8XX4Z7J3d4wS2LvO5VkZwmaGWdPrODua8/j6a9ezn9devrQjYjIcBPocK/37vXy2UWpb0sQz8wYV5H6Bkzzp1dRHAr0SyYiAVGYb544Q2ZOHsPu//37A9rmK1fNYlJlGff9S9KvjBUR8QXfdkOXnD0u5/s0oLS4iC9dotMhZfjTRUySjm/DfXJlWdLl8TcOG1Wa+syWa86fkvM6iYgMF74N91S3rp07Lfrh6aO3LKFqVOpvnbmg9vRbCOi2K+InOlwlHd+OuacK93uu+z0+Pe8slmS4/0uyL+o1/bqISED4tueeqpddHCrKGOwD3aeIiN/4Ntz1jUIiIqn5NtyV7SIiqfku3GOhPtSee9KrUYe0RxGR4cN34R4z1J57sg9Ule7iJzrPXdLxbbjnY8y9siz1qZMiIn7iu1MhYz3uJDeEHLTN376OitJiQrncqYhIAfkv3L3HXPbcK8vVYxf/UVdE0vHdsEzE67qbTpcREUnJh+EefVS2i4ik5rtwj90YTMPjIiKp+S7ceyOxcB9auq/I8gs8RET8yLfhXlw0tKrPmFCRi+qIFIzOc5d0fBfuPV64l4Q0LiMikorvToXsG5bJwaD7j26/kONdPUPej4jIcOO7cI99oJqLfvtVv6dvYxKRYPLdsMypK1Q1LCMjm34DJJ2swt3MlplZk5k1m9mqJOvPNrMNZvaWmb1jZtfnvqpROs9dRCSzjOFuZiHgEWA5MBe42czmJhT778Bq59xi4CbgB7muaIxDV6iKiGSSTc/9IqDZOdfinDsJPAHckFDGAWO96SpgX+6q2F9fzz1fTyAiEgDZfKA6HdgbN98KXJxQ5tvAr83sq8BoYGmyHZnZSmAlQHV1NeFweIDVhd27TwKw5/3dhMOD/xuS7XMPpo750NHRMWzqcqaozent2RP9XWhpaSFsrXmsVX7pfc6PbMI9WSc58fqJm4EfO+e+Z2aXAv/PzOY75yL9NnKuEWgEqK+vdw0NDQOu8OtdTbCzmbq6OhoaZme/4bpn+s1mfG6v/GDqmA/hcHjY1OVMUZvT29i5HVp2MnPmTBoaZuW3Ynmk9zk/shmWaQVmxM3XcPqwyx3AagDn3MtAOTApFxVM1Dfmno+di4gERDbh/how28zqzKyU6AemaxLK7AGuATCzOUTD/UAuKxrTdyqk7hwmIpJSxnB3zvUAdwHrgW1Ez4rZYmbfMbMVXrFvAHea2Sbg58BtziX9ltIhi+iGGiIiGWV1hapzbi2wNmHZ/XHTW4HLclu1FHUhN3eFFPE7/QZIOr69QlXZLiKSmu/CPUbZLiKSmu/Cve/GYUp3EZGUfBfup65QHVi6f/uzc/nq1f49F1gkkc4tkHR8eMvf6ONAe+63XVYHwD+80JzjGomIDD8+7LnrxmEiIpn4LtxjdA2TiEhqvgv3SA6/iUlEJKh8F+6nxtwV7zKy6TdA0vFfuPddoVrgioiIDGO+C/e+e8uo5y4ikpLvwv3UF2QXth4ihabz3CUdH4Z77ANVpbuISCo+DPfoo0ZlRERS81+465uYREQy8l+4q+cuIpKR/8Lde9R57iIiqfku3HWFqkiUfgckHd+FO32nQurQFhFJxXfhHtGXdYiIZOS7cNcFqiJRuohJ0vFfuA/ym5hEREYS/4W796ieu4hIar4LdxERyUzhLiISQAp3EZ/SyKSk47tw1wEtIpKZ78JdREQy812469xekSj9Lkg6vgt3ERHJTOEuIhJACncRkQBSuIuIBFBW4W5my8ysycyazWxVijJfMLOtZrbFzH6W22qKiMhAFGcqYGYh4BHgWqAVeM3M1jjntsaVmQ3cC1zmnDtkZlPyVWERidI1H5JONj33i4Bm51yLc+4k8ARwQ0KZO4FHnHOHAJxzbbmt5inO6QQwEZFMMvbcgenA3rj5VuDihDLnAZjZfwAh4NvOuXWJOzKzlcBKgOrqasLh8IAr3NbWCcC2bduoOrxjwNvHZPvcg6ljPnR0dAybupwpanN6779/EoCWlhbC1prHWuWX3uf8yCbck/33l9h9LgZmAw1ADfBvZjbfOXe430bONQKNAPX19a6hoWGg9eWf9r0JH+xnzpw5NHxi+oC3Z90zAGR87mzLnSHhcHjY1OVMUZvT29i5HXbtZObMmTQ0zMpvxfJI73N+ZDMs0wrMiJuvAfYlKfMr51y3c24X0EQ07HPOdCN3EZGMsgn314DZZlZnZqXATcCahDL/AlwFYGaTiA7TtOSyoiIikr2M4e6c6wHuAtYD24DVzrktZvYdM1vhFVsPtJvZVmADcI9zrj1flRYRkfSyGXPHObcWWJuw7P64aQfc7f2IiEiB+e4KVZ0KKSKSme/CPUYfrMpIp98ASce34S4iIqkp3EV8SgOUko7CXUQkgLI6W2Ykqh5bxnVzzyp0NUREBkXhnsLGv1xa6CqIiAyab4dldEqkiEhqvg13ERFJzbfhrvPcRURS8224i4x06t5IOgp3EZ/Sp06SjsJdRCSAFO4iIgGkcBcRCSDfhbvGGUVEMvNduMfoTAERkdR8G+4iIpKawl1EJIB8G+4ae5eRTkOTko7vwl0HtEiUOjiSju/CXUREMvNduKu3IiKSme/CPUbDMyIiqfk23EVEJDWFu4hIACncRUQCSOEuIhJACncRkQDyX7jrXEgRkYz8F+4efT+2iEhqvg13ERFJzbfh7jQ8IyKSUlbhbmbLzKzJzJrNbFWacjeamTOz+txVMfFJ8rZnEZHAyBjuZhYCHgGWA3OBm81sbpJylcDXgI25rqSIiAxMNj33i4Bm51yLc+4k8ARwQ5Jy/wN4EOjMYf1ERGQQirMoMx3YGzffClwcX8DMFgMznHNPm9mfp9qRma0EVgJUV1cTDocHXOG2tujfjq1bt1J56L0Bbx8zmOcupI6ODt/VeajU5vT2vH8SgF0tLYStNY+1yi+9z/mRTbgnG+Xu+zjTzIqAvwduy7Qj51wj0AhQX1/vGhoasqpkvF/+7k34YD/z5s2lYeG0AW/PumcAGMxzF1I4HPZdnYdKbU5vY+d22LWTupkzaWiYld+K5ZHe5/zIZlimFZgRN18D7IubrwTmA2Ez2w1cAqzJ64eqIiKSVjbh/how28zqzKwUuAlYE1vpnDvinJvknKt1ztUCrwArnHOv56XGIiKSUcZwd871AHcB64FtwGrn3BYz+46Zrch3BUVEZOCyGXPHObcWWJuw7P4UZRuGXi0RERkK316hKiIiqfku3J1uCykikpHvwl1ERDLzXbibbi4jAsAfLKkhVGR8ZuHUQldFhqGsPlAVkeFn1pQx7Pyr6wtdDRmmfNdzFxGRzBTuIiIBpHAXEQkg34W7ToUUEcnMd+Eeo7NmRERS8224i4hIagp3EZEAUriLiASQwl1EJIB8G+46a0ZEJDXfhruIiKTm23DXqZAiIqn5NtxFRCQ1hbuISAAp3EVEAkjhLiISQAp3EZEA8l24O53eLiKSke/CPcZ0JqSISEq+DXcREUlN4S4iEkC+DXeNvYuIpOa7cNdYu4hIZr4LdxERycx34a7hGBGRzHwX7jEanhERSc234S4iIqkp3EVEAiircDezZWbWZGbNZrYqyfq7zWyrmb1jZs+b2Tm5r6qIiGQrY7ibWQh4BFgOzAVuNrO5CcXeAuqdcwuBfwIezHVFRUQke9n03C8Cmp1zLc65k8ATwA3xBZxzG5xzH3uzrwA1ua2miIgMRHEWZaYDe+PmW4GL05S/A3g22QozWwmsBKiuriYcDmdXyzhtBzoB2LJlCxXtTQPePmYwz11IHR0dvqvzUKnNI4PanB/ZhHuykw6Tnm1uZrcA9cCVydY75xqBRoD6+nrX0NCQXS3jPLH3DfjwA+bPm0fDgqkD3p51zwAwmOcupHA47Ls6D5XaPDKozfmRTbi3AjPi5muAfYmFzGwp8E3gSudcV26ql5quZRIRSS2bMffXgNlmVmdmpcBNwJr4Ama2GHgMWOGca8t9NeOfK597FxEJhozh7pzrAe4C1gPbgNXOuS1m9h0zW+EV+xtgDPBLM3vbzNak2J2IiJwB2QzL4JxbC6xNWHZ/3PTSHNdLRESGQFeoiogEkO/CXXeFFBHJzHfhHqPPVUVEUvNtuIuISGoKdxGRAFK4i4gEkMJdRCSAFO4iIgHku3AvL4lWOVSk82VERFLJ6grV4eTbK+bRc/QA18ypLnRVRESGLd/13MdVlHLjeaXquYuIpOG7nruISDLd3d20trbS2dlZ6KpkVFVVxbZt29KWKS8vp6amhpKSkkE9h8JdRAKhtbWVyspKamtrsWF+b/Bjx45RWVmZcr1zjvb2dlpbW6mrqxvUc/huWEZEJJnOzk4mTpw47IM9G2bGxIkTh/RfiMJdRAIjCMEeM9S2KNxFRAJI4S4iEkAKdxGRHPnc5z7HBRdcwLx582hsbARg3bp1LFmyhEWLFnHNNdcA0NHRwe23386CBQtYuHAhTz75ZM7rorNlRCRwHvjXLWzddzSn+5w7bSzf+uy8tGUef/xxJkyYwIkTJ7jwwgu54YYbuPPOO3nppZeoq6vj4MGDADz44INUVVWxefNmAA4dOpTTuoLCXUQkZx566CGeeuopAPbu3UtjYyNXXHFF3+mMEyZMACAcDrN69eq+7caPH5/zuijcRSRwMvWw8yEcDvPcc8/x8ssvU1FRQUNDA4sWLaKpqem0ss65vJ/ZozF3EZEcOHLkCOPHj6eiooLt27fzyiuv0NXVxYsvvsiuXbsA+oZlrr76ah5++OG+bfMxLKNwFxHJgWXLltHT08PChQu57777uOSSS5g8eTKNjY18/vOfZ9GiRXzxi18E4J577uHQoUPMnz+fRYsWsWHDhpzXR8MyIiI5UFZWxrPPPpt03fLly/vNjxkzhp/85Cd5rc+IC/e/uXEh50wcXehqiIjk1YgL9/9UP6PQVRARyTuNuYuIBJDCXUQCwzlX6CrkzFDbonAXkUAoLy+nvb09EAEfu597eXn5oPcx4sbcRSSYampqaG1t5cCBA4WuSkadnZ0Zgzv2TUyDpXAXkUAoKSkZ9LcWnWnhcJjFixfn9Tk0LCMiEkAKdxGRAFK4i4gEkBXqk2UzOwC8P8jNJwEf5bA6fqA2jwxq88gwlDaf45ybnKlQwcJ9KMzsdedcfaHrcSapzSOD2jwynIk2a1hGRCSAFO4iIgHk13BvLHQFCkBtHhnU5pEh72325Zi7iIik59eeu4iIpOG7cDezZWbWZGbNZraq0PUZCjN73MzazOzduGUTzOw3ZrbDexzvLTcze8hr9ztmtiRum1u98jvM7NZCtCUbZjbDzDaY2TYz22Jmf+YtD3Kby83sVTPb5LX5AW95nZlt9Or/CzMr9ZaXefPN3vrauH3d6y1vMrNPF6ZF2TOzkJm9ZWZPe/OBbrOZ7TazzWb2tpm97i0r3LHtnPPNDxACdgIzgVJgEzC30PUaQnuuAJYA78YtexBY5U2vAv7am74eeBYw4BJgo7d8AtDiPY73pscXum0p2jsVWOJNVwLvAXMD3mYDxnjTJcBGry2rgZu85Y8C/8Wb/lPgUW/6JuAX3vRc73gvA+q834NQoduXoe13Az8DnvbmA91mYDcwKWFZwY7tgr8gA3zxLgXWx83fC9xb6HoNsU21CeHeBEz1pqcCTd70Y8DNieWAm4HH4pb3Kzecf4BfAdeOlDYDFcCbwMVEL2Ap9pb3HdfAeuBSb7rYK2eJx3p8ueH4A9QAzwNXA097bQh6m5OFe8GObb8Ny0wH9sbNt3rLgqTaObcfwHuc4i1P1XZfvibev96LifZkA91mb3jibaAN+A3RHuhh51yPVyS+/n1t89YfASbiszYD3wf+Aoh48xMJfpsd8Gsze8PMVnrLCnZs++2Wv5Zk2Ug53SdV2333mpjZGOBJ4OvOuaNmyZoQLZpkme/a7JzrBT5hZuOAp4A5yYp5j75vs5l9Bmhzzr1hZg2xxUmKBqbNnsucc/vMbArwGzPbnqZs3tvst557KxD/Ddc1wL4C1SVfPjSzqQDeY5u3PFXbffWamFkJ0WD/qXPun73FgW5zjHPuMBAmOsY6zsxinav4+ve1zVtfBRzEX22+DFhhZruBJ4gOzXyfYLcZ59w+77GN6B/xiyjgse23cH8NmO196l5K9MOXNQWuU66tAWKfkN9KdFw6tvyPvE/ZLwGOeP/mrQeuM7Px3ifx13nLhh2LdtH/EdjmnPu7uFVBbvNkr8eOmY0ClgLbgA3AjV6xxDbHXosbgRdcdPB1DXCTd2ZJHTAbePXMtGJgnHP3OudqnHO1RH9HX3DO/WcC3GYzG21mlbFposfkuxTy2C70hxCD+NDieqJnWewEvlno+gyxLT8H9gPdRP9i30F0rPF5YIf3OMEra8AjXrs3A/Vx+/ky0Oz93F7odqVp7+VE/8V8B3jb+7k+4G1eCLzltfld4H5v+UyiQdUM/BIo85aXe/PN3vqZcfv6pvdaNAHLC922LNvfwKmzZQLbZq9tm7yfLbFsKuSxrStURUQCyG/DMiIikgWFu4hIACncRUQCSOEuIhJACncRkQBSuIuIBJDCXUQkgBTuIiIB9P8BCpD4h7Q3CykAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['acc'] = training_accuracy\n",
    "df.plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------NEURAL NETWORKS------------------------------------\n",
      "---------------------Concatenation-------------------------------------\n",
      "Errors: 15  Correct :335\n",
      "Testing Accuracy: 95.71428571428572%\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "predictedTestLabelList = []\n",
    "\n",
    "\"\"\n",
    "for i,j in zip(processedTestingLabel,predictedTestLabel):\n",
    "    #predictedTestLabelList.append(decodeLabel(j))\n",
    "    \n",
    "    if np.argmax(i) == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print ('--------------------NEURAL NETWORKS------------------------------------')\n",
    "print ('---------------------Concatenation-------------------------------------')\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100) + \"%\")\n",
    "print('-------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Placeholder\n",
    "inputTensor  = tf.placeholder(tf.float32, [None, 512])\n",
    "outputTensor = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_NEURONS_LAYER_1 = 100\n",
    "NUM_HIDDEN_NEURONS_LAYER_2 = 100\n",
    "\n",
    "LEARNING_RATE = 1\n",
    "\n",
    "# Initializing the weights to Normal Distribution\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape,stddev=0.01))\n",
    "\n",
    "#2 hidden layer\n",
    "input_hidden_weights  = init_weights([512, NUM_HIDDEN_NEURONS_LAYER_1])\n",
    "\n",
    "#initializing the hidden layer 1 to output layer weights 1\n",
    "hidden_output_weights1 = init_weights([NUM_HIDDEN_NEURONS_LAYER_1, NUM_HIDDEN_NEURONS_LAYER_2])\n",
    "\n",
    "#initializing the input to hidden layer 2 weights\n",
    "input_hidden_weights2 = init_weights([NUM_HIDDEN_NEURONS_LAYER_1, NUM_HIDDEN_NEURONS_LAYER_2])\n",
    "\n",
    "#initializing the output to hidden layer 2 weights\n",
    "hidden_output_weights2 = init_weights([NUM_HIDDEN_NEURONS_LAYER_2, 1])\n",
    "\n",
    "# Computing values at the hidden layer 1\n",
    "hidden_layer1 = tf.nn.relu(tf.matmul(inputTensor, input_hidden_weights))\n",
    "\n",
    "#computing values at the hidden layer 2\n",
    "hidden_layer2 = tf.nn.relu(tf.matmul(hidden_layer1, input_hidden_weights2))\n",
    "\n",
    "# Computing values at the output layer\n",
    "output_layer = tf.matmul(hidden_layer2, hidden_output_weights2)\n",
    "\n",
    "# Defining Error Function\n",
    "error_function = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_layer, labels=outputTensor))\n",
    "\n",
    "# Defining Learning Algorithm and Training Parameters\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(error_function)\n",
    "\n",
    "# Prediction Function\n",
    "prediction = tf.round(tf.sigmoid(output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8e106ce1314369b5f1031c27f5fe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_EPOCHS = 5000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "processedTrainingData = np.array(diff_train_data)\n",
    "processedTrainingLabel = np.array(diff_train_vector)\n",
    "processedTestingData = np.array(diff_test_data)\n",
    "processedTestingLabel = np.array(diff_test_vector)\n",
    "\n",
    "training_accuracy = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Set Global Variables ?\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(NUM_OF_EPOCHS)):\n",
    "        \n",
    "        #Shuffle the Training Dataset at each epoch\n",
    "        p = np.array(np.random.permutation(range(len(processedTrainingData)-1)),dtype='int')\n",
    "        processedTrainingData  = processedTrainingData[p]\n",
    "        processedTrainingLabel = processedTrainingLabel[p]\n",
    "        \n",
    "        # Start batch training\n",
    "        for start in range(0, len(processedTrainingData), BATCH_SIZE):\n",
    "            end = start + BATCH_SIZE\n",
    "            sess.run(training, feed_dict={inputTensor: processedTrainingData[start:end], \n",
    "                                          outputTensor: processedTrainingLabel[start:end]})\n",
    "        # Training accuracy for an epoch\n",
    "        training_accuracy.append(np.mean(np.argmax(processedTrainingLabel, axis=1) ==\n",
    "                             sess.run(prediction, feed_dict={inputTensor: processedTrainingData,\n",
    "                                                             outputTensor: processedTrainingLabel})))\n",
    "    # Testing\n",
    "    predictedTestLabel = sess.run(prediction, feed_dict={inputTensor: processedTestingData})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x248a2bf1588>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHjNJREFUeJzt3XuUVOWd7vHvr7uB5mZz01ZptHFCNKAg0hIyZkzHW8BEyeQmzGTiZBw5M4lnTpIzZuky0YyZtZKYk8Q4mhP7TJyYOccLo8mEKJdoQmmciIAXRC6tLRBpMHIVaKBpunnPH7W7qa6uy66qXVW9dz2ftVhdtffbu35vrebpt9+9693mnENERKKlqtwFiIhI8BTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIJqyvXCEyZMcI2NjXl97+HDhxk5cmSwBQ1y6nNlUJ8rQyF9fvHFF/c4507N1q5s4d7Y2MjatWvz+t5YLEZzc3OwBQ1y6nNlUJ8rQyF9NrM/+GmnaRkRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYmgrOFuZg+Y2S4zey3NfjOze8yszcxeNbOLgi9TRERy4Wfk/lNgbob984Ap3r9FwP8uvCwRESlE1uvcnXPPmlljhibzgZ+5+P36VpnZGDM7wzn3dkA19rNm2z5+/kYXG2njsvNO47zTTynGywxa2/cd4bEX26mE2yNu+0MXL3W1lruMklKfK8OYoz00F/k1gvgQ00Rge8Lzdm/bgHA3s0XER/fU19cTi8VyfrGlW7tY8uZxlrzZyl3LW/np3Mr4ZFtHRwexWIzHXu/iiS3HsXIXVBIO3mwrdxElpj5Xgs/8icsr/3IRRLinypmUw0rnXAvQAtDU1OTy+YRWczP86mtPcrS793nuxwij3k+0rTq6maFvbeX1f55X7pKKTp9crAzqc3EEcbVMOzAp4XkDsDOA46bVG+yVyKX+vSki0k8Q4b4E+Jx31cwc4ECx5ttFRMQfP5dCPgw8D5xrZu1mdoOZ/Z2Z/Z3XZCmwBWgD/g/whaJV69M7BztpvOVJlr8Wwd8xLvU8mIhIIj9XyyzMst8BXwysogBsfPsgAA+v3s7c888oczUiIqUXyk+ofnhS2VYqHhRMQ3cRySKU4X76SH9lR/HUYxT7JCLBC2W4h7LoAJlm3UUki1DmZLmmJV7bcYDp31jB0a6e8hQAFfHJVBEpXCTDvdDs7+45wYadB/ptW99+gI/9y3Mc7OzmkTVv8a2lm5j3w98NCNuOY9388xMbeWvvEQ52Hh9w7Be27GX7viMF1ac5dxHJJpzh7rNdvqPc7/66lY/e8xxvvHOob9s19z7X9/jZ13dz/7Nb2PT2QbbvO9rve+/9bRv/+txWLv3uSq78/jMDjn1dyyr+7K6VedUFoIG7iPgRynCvKvLIdd32dwHY3XEs5f6Vrbv7Hl/63ZXc8cuTqyEnjsrfOXiMrzz6Sk6v3XPC0dV9AoC7n36dB3+/jY5j3Szbepz17QfYf+Q4R8o4LSQi4RDKcPc7LWEBzF88+/pupt2+PGObB58/eTPy32x+p9++n7+8g87jPb7/iljYsor3fm0ZAHc//QZ3LNnA+Xes4NHWLq659zkef6k9xx6ISCUKZbj7LTrfaZnEb/veU69z2MdI+XhPfLTdefzEgH3nfX0531622ddrr962D4AX/7DPV3sRkVRCGe4lPaHo8xfElNuWMffuZ9Puv//ZLTm97P9d9VZO7UVEEoUz3MtdQBqb/3goeyMRkRIIZ7hnGboHMddeDF99bJ3vtkvWFXXVZBGJuFAu0lLK30hBXnm4eG36k6HOOdp2dfQ97zmhax5FJH+hHLkX+1LIPg6O95QmZH/5yk6u/EH6OXsRkVyEMtxLNety/b+tZpO3fHAqC2efFdhrbczwOiIiuQpnuBe4369so/ajXcHd72+QniYQkZAKZ7gPkiAcWpP+7fvoBbndJEQrPYpIkEIZ7iWbc8+ipjr92zd+1NCcjqXVHkUkSKEMd/8LhxW1DKoz/AmR6++fXD/kJCKSSSjDvdgjd7+/EwqZHupdHExEpBhCGe5+MzXf8F29Nfu6Lhc3jqVu+JD8XgDo6lG4i0jxhDPcfaZ2saZlZjTU8fCNc7hmxpl5H6OnRNfPi0hlCmW4l/uE6tCaqownU/3oPqGRu4gUTyjDvdwXy/T+RdA4fiQXThrD4v/2gZyP0ZPHnxX3/9WsnL9HRCpTONeWyXYP1RzT3znHN5Zs4JOzGpjeMCZ7e+/r0Joq/vOLl+T2Yp7etWPWbtvHM6/vztI67iPTTs/rtUSk8oQy3IP+ENPhrh4efP4PPPZiOxvunJu1fRDXpPeG+6d+/HzBxxIRSaZpmQRBn+K86cPvSbtPqz6KSDGFM9x9prvLI64zLRR28rj+/ONHzk1/DGW7iBRRKMM9W9GFrNNy00MvZW1TjmC+/4oRpX9REQmtUIZ70HPuvYdzDt7cfThr+3IMuofVlPsaIREJE1/hbmZzzazVzNrM7JYU+88ys5Vm9rKZvWpmVwdf6knlvs49CA442Hm83GWISERlDXczqwbuA+YBU4GFZjY1qdnXgMXOuZnAAuBHQRfar6aEx4fKEZABzct8e9nmQI4jIpLMz8h9NtDmnNvinOsCHgHmJ7VxwCne4zqgqHd3Tlx+4OjxnmK+VEpBTcscO65PqYpIcfgJ94nA9oTn7d62RN8APmtm7cBS4L8HUp0PQd7kwu8vir+ac3bBr+Wcy3ru4LTRwwp+HRGpTH4+xJQqgpIHrwuBnzrnvmdmHwD+3czOd871G5qa2SJgEUB9fT2xWCyPkuHIkSN9ZT3//O85ZWj/EjfsiYf0vn37fb3Gse7cxuKndrxJLPZmxjbtO3YQi+1Ju3/16tW888fMU0oj7eT+jo6Ofn3J970Lk+Q+VwL1uTKUos9+wr0dmJTwvIGB0y43AHMBnHPPm1ktMAHYldjIOdcCtAA0NTW55ubmvIre/qvfAkcB+OAllzBuZP+7HtW8sQfWvsDYsWNpbp6T9Xirt+6Dp/1/UjRl3cufBGDyhJFs3XOYhokTaW4+v297stmzZ/PS0TdhR3va16mrOwUOvgvAqFGj4q/rHS/f9y5MYrFYRfQzkfpcGUrRZz/TMmuAKWY22cyGEj9huiSpzVvA5QBm9j6gFvC3YEqBgpiU+ePBzrT7/uuWy3I61l/MPst322xTSv+ycGZOry0i0itruDvnuoGbgBXAJuJXxWwwszvN7Fqv2f8EbjSzdcDDwF+7Et0UtCrFxHW2uewvPvQSy9a/7ev4Q7zrLhdcPClLS68en9dpOrLXOWmcPrgkIvnxtXCYc24p8ROlidtuT3i8EchvecRCZQjIdL9ennz1bZ589W22ffujXrv0v4fGjxrGK7dfyahhNTyyZnvadj7KEREpmVCuCllK1VXGmBHxOf1ffvESOrNcUeP307Ov7TgQ+CdtRUR6hXL5gX5KuBbAjEljeP854zO2SZfXZ9bV9nv+k+e2ZmgtIlKY8Id7CrlGZinODiTf97W6yjRyF5GiCWW4J86Rp1rWtxwLe13xvtMy7k8O8pooLJAjIoNWKOfcB+NS6D/6y1l0HOtmySs7Uu5PDncL9LO1IiL9hTLcE6WaUsl5WiaAXxdDa6oYVzM0e0MRkRIIfbiHRfI43ZF9bRmAxvEjmHX2OGB/cQoTkUgKfbgXOuY+cOQ4X350XSC1ZJIc5F09/iqP3fzh+NcKW3tDRAoTynD3e3WLn+mWLz36coHV5Gfd9ndZt/3dsry2iERfKK+WCZKf2+oFQSdPRaSUQh/uhS5hE8TJVD+Sr3MXESmm0Id7Sjnk6AndDElEIiiU4e7SPB7QzsegfMe7RwstxxcN3EWklEIf7vk1iNvTcazQUkREBqVQhnuiQqbcj3aV7ubaGriLSCmFPtxT8Zv3x7pLN+GuE6oiUkqhvM49Mb1TLhzmM90X/WxtQAWddN3FZ7Fh50G+fOV7+21XtItIKYUy3IO6eHHLnuCvcR8+tJrvfnrGgO0auItIKYV/WiZF0pfq2vVi+uwc/zfaFhFJFv5w92nXwU5efmt/wR96yldXjvP7Y4ZrhUkRyV/owz3V8gHJ+b2+/QBXfP8Z/vxHvy9RVQNt23skp/aaxhGRQoQy3BOze/2O9ItvOeC3m9/hmnuf42Bnd3xb+GdsRESyCuUJ1UQnUs65n7R1T24j5l4P/s1sxgwfkl9RAdDAXUQKEcpwTwy+E1mG4skh+W+/3+brNT703lNzqklEZDAJ/bRMqmxPPGn60Oq3+u27+6nXi1RVwDTpLiIFCOXIPdGJVPMyvRy07erot+nQse6Mx1t3x1WZj1kEo4fVDKhL0S4ihQh/uGeZc89VXRnm2XOpd8WXLmWvFjwTkSxCH+51w1N0oTctQzL8zeXa+3NPHw2MLl4xIhIJoZxzT3T2+JHpd/rMzIf+9v0A3Hb1+wKoKLWrLzg9p/aacheRQoRz5J4Q2t0p5mV6lx/o8Tki/tP3TGDbtz8aSGnpVFel/z2aqkoLy58dIjIohX7k3pPhPnk9JT4xmkmmqNYHq0QkaL7C3czmmlmrmbWZ2S1p2nzGzDaa2QYzeyjYMtPrSZHtvWE5mMI9V5qWEZFCZJ2WMbNq4D7gSqAdWGNmS5xzGxPaTAFuBS5xzu03s9OKVXCyPx7sTLsv1ZRNsnEjS7NAV6awjsIqliIyuPgZuc8G2pxzW5xzXcAjwPykNjcC9znn9gM453YFW2Z633xi44BtJ0fu2Vdi/MTMiUGXlNI5E0al3ZdqWkYDdxEphJ8TqhOB7QnP24H3J7V5L4CZ/RdQDXzDObc8+UBmtghYBFBfX08sFsujZDhy9CiJ8Zd8nPW74h8IOtiR/WYcW9/aTixW/N9F7769Le2+xF9C546tonX/CbZu20ostqNve0dHR97vV1ipz5VBfS4OP+GeahCZPNasAaYAzUAD8DszO98512/JRudcC9AC0NTU5Jqbm3OtF4A3fvEb4OR0TPJxlix+BdjBsNrhcDjzwmHT3tNIc/O5edXhy/InATj33HNhw/qUTaqsii80T2bamXVsfPsArSvf5JzJk2luntLXJhaLDehn1KnPlUF9Lg4/4d4OTEp43gDsTNFmlXPuOLDVzFqJh/2aQKpMkmnKYvehY/z8pfiIt7sn+1z2Fy97T0BVZZbtBOlX554HwIadB0pQjYhEnZ859zXAFDObbGZDgQXAkqQ2/wl8GMDMJhCfptkSZKGJMkX28YTLZw52Hs94nPkXnsmwmuqAqspf4gnVvg/X6nIZESlA1nB3znUDNwErgE3AYufcBjO708yu9ZqtAPaa2UZgJXCzc25vsYrOJDETD3VmXiSsepAEaOIJ1ZmTxgBwwcS6MlUjIlHg6xOqzrmlwNKkbbcnPHbAV7x/ofHzl3fw/esuLMlrZfrEaXXVyX1XTTudVbdezul1taUoS0QiKpzLD2RwtKun3CX0861PXJD1hiLXzjiz33MFu4gUKnLhftn3nil3Cf0snH0WAIvXbE+5/6kvX8pZ40eUsiQRqQChD/dcV1scbKbUa/leEQle6BcO87Po1uZvzuU7n7xgwPYJo0qz9ACgj5yKSEmFPtz9qB1SzXUXn8W6O67iU7MaylrLNUnz6yIixRD6cM9ludy64UP4X5+ekdf3BqVKI3gRKYHwh3sBKyrOOWd8gJX4o2wXkVIIZbgnxnkho+9TUt1/tUhShfp5p+tkqogURyjDPVG2bD8zwzXjVWX4hGrisgL3/sXMkr++iFSGUIZ7YiRnGrk3nT2WJ//hzwZsv/4DZwOlDfehNfG3eljNybd83+HMa9+IiOQrlOHudybme5+ZwdgUd1qaPGEkUNpb2X30gjP4QvOfcOvV7+vbFubbAIrI4Bb6DzFlivp0I/O+lReLUE06NdVVfcv69qrWpTMiUiShHLknyueEau/3lHtZ3erQv/siMliFPl5+98YeuntO0NU98H6p6bLbZdkvIhJ2oZ+W6eo5wZ//6Pes33GA0bX9u5NuZO68oXumZXhLoRwfohKRyhDKkXtyKK7fEb81Xbabcww2U07Tde4iUhyhDHe/0o3LT865l6yUlOpGDClvASISWaEMd7+hnH7OvXdaRkQkmkI5517oXPWY4fFr38ePGhZANblb+Y/NnFIbyrdeREKiIhPmU7MacDg+eVF5lv/t/RCViEixhDLch1Vn3j/r7LFMO/MUTj8l9boyVVXGdRefVYTKREQGh1CGe2Nd5nR//O//tESViIgMTqE8oZrKVVPry12CiMigEcqRe6Kt37qaQ8e62dfRxa83vsNZ40aUuyQRkbIL/cjdzDildkjfZY+F3JlJRCQqQh/uvcq9lICIyGASmXAXEZGTIhfuWoxLRCQCJ1R7TRw7nPkXnsnffvCccpciIlJ2kQn36irjhwt0w2kREfA5LWNmc82s1czazOyWDO0+ZWbOzJqCK1FERHKVNdzNrBq4D5gHTAUWmtnUFO1GA/8AvBB0kSIikhs/I/fZQJtzbotzrgt4BJifot03gbuAzgDrExGRPPgJ94nA9oTn7d62PmY2E5jknHsiwNpERCRPfk6opvp0UN8Fh2ZWBfwA+OusBzJbBCwCqK+vJxaL+SoyWUdHR19Z+R4jbDo6Oiqmr73U58qgPheHn3BvByYlPG8AdiY8Hw2cD8S8G1KfDiwxs2udc2sTD+ScawFaAJqamlxzc3NeRcfflMMA5HuMsInFYhXT117qc2VQn4vDz7TMGmCKmU02s6HAAmBJ707n3AHn3ATnXKNzrhFYBQwIdhERKZ2s4e6c6wZuAlYAm4DFzrkNZnanmV1b7AJFRCR3vj7E5JxbCixN2nZ7mrbNhZclIiKFiNzaMiIionAXEYkkhbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEIUriLiESQr3A3s7lm1mpmbWZ2S4r9XzGzjWb2qpn9xszODr5UERHxK2u4m1k1cB8wD5gKLDSzqUnNXgaanHPTgceAu4IuVERE/PMzcp8NtDnntjjnuoBHgPmJDZxzK51zR7ynq4CGYMsUEZFc1PhoMxHYnvC8HXh/hvY3AMtS7TCzRcAigPr6emKxmL8qk3R0dAAGkPcxwqajo6Ni+tpLfa4M6nNx+Al3S7HNpWxo9lmgCfhQqv3OuRagBaCpqck1Nzf7qzJJ/E05DEC+xwibWCxWMX3tpT5XBvW5OPyEezswKeF5A7AzuZGZXQHcBnzIOXcsmPJERCQffubc1wBTzGyymQ0FFgBLEhuY2UzgfuBa59yu4MsUEZFcZA1351w3cBOwAtgELHbObTCzO83sWq/Zd4FRwH+Y2StmtiTN4UREpAT8TMvgnFsKLE3adnvC4ysCrktERAqgT6iKiESQwl1EJIIU7iIiEaRwFxGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISATVlLsAEZEgHD9+nPb2djo7O8tdSlZ1dXVs2rQpY5va2loaGhoYMmRIXq+hcBeRSGhvb2f06NE0NjZiZuUuJ6NDhw4xevTotPudc+zdu5f29nYmT56c12toWkZEIqGzs5Px48cP+mD3w8wYP358QX+FKNxFJDKiEOy9Cu2Lwl1EJIIU7iIiEaRwFxEJyMc//nFmzZrFtGnTaGlpAWD58uVcdNFFzJgxg8svvxyAjo4OPv/5z3PBBRcwffp0Hn/88cBr0dUyIhI5//SrDWzceTDQY0498xTuuGZaxjYPPPAA48aN4+jRo1x88cXMnz+fG2+8kWeffZbJkyezb98+AO666y7q6upYv349APv37w+0VlC4i4gE5p577uEXv/gFANu3b6elpYVLL72073LGcePGARCLxVi8eHHf940dOzbwWnyFu5nNBX4IVAP/6pz7dtL+YcDPgFnAXuA659y2YEsVEfEn2wi7GGKxGE8//TTPP/88I0aMoLm5mRkzZtDa2jqgrXOu6Ff2ZJ1zN7Nq4D5gHjAVWGhmU5Oa3QDsd869B/gB8J2gCxURGcwOHDjA2LFjGTFiBJs3b2bVqlUcO3aMZ555hq1btwL0Tctcdtll3HvvvX3fW4xpGT8nVGcDbc65Lc65LuARYH5Sm/nAg97jx4DLLUoXnIqIZDF37ly6u7uZPn06X//615kzZw6nnnoqLS0tfOITn2DGjBlcd911ANx8883s37+f888/nxkzZrBy5crA6/EzLTMR2J7wvB14f7o2zrluMzsAjAf2BFGkiMhgN2zYMJYtW5Zy37x58/o9HzVqFA8++GDKtkHxE+6pRuAujzaY2SJgEUB9fT2xWMzHyw/U0dHB56YOY3JdVd7HCJuOjo6K6Wsv9bkyBNXnuro6Dh06VHhBJdDT0+Or1s7OzrzfGz/h3g5MSnjeAOxM06bdzGqAOmBf8oGccy1AC0BTU5Nrbm7Oo+T4iYs7P5bf94ZVLBYj3/crrNTnyhBUnzdt2pRxMa7BJNvCYb1qa2uZOXNmXq/hZ859DTDFzCab2VBgAbAkqc0S4Hrv8aeA3zrnBozcRUSkNLKO3L059JuAFcQvhXzAObfBzO4E1jrnlgA/Af7dzNqIj9gXFLNoEZFUSnGJYakUOj72dZ27c24psDRp2+0JjzuBTxdUiYhIAWpra9m7d28klv3tXc+9trY272PoE6oiEgkNDQ20t7eze/fucpeSVWdnZ9bg7r0TU74U7iISCUOGDMn7rkWlFovF8j5R6pdWhRQRiSCFu4hIBCncRUQiyMp1ObqZ7Qb+kOe3T6DyljZQnyuD+lwZCunz2c65U7M1Klu4F8LM1jrnmspdRympz5VBfa4MpeizpmVERCJI4S4iEkFhDfeWchdQBupzZVCfK0PR+xzKOXcREcksrCN3ERHJIHThbmZzzazVzNrM7JZy11MIM3vAzHaZ2WsJ28aZ2VNm9ob3day33czsHq/fr5rZRQnfc73X/g0zuz7Vaw0GZjbJzFaa2SYz22Bm/8PbHuU+15rZajNb5/X5n7ztk83sBa/+R73ltDGzYd7zNm9/Y8KxbvW2t5rZR8rTI//MrNrMXjazJ7znke6zmW0zs/Vm9oqZrfW2le9n2zkXmn/Elxx+EzgHGAqsA6aWu64C+nMpcBHwWsK2u4BbvMe3AN/xHl8NLCN+16s5wAve9nHAFu/rWO/x2HL3LU1/zwAu8h6PBl4nftP1KPfZgFHe4yHAC15fFgMLvO0/Bv7ee/wF4Mfe4wXAo97jqd7P+zBgsvf/oLrc/cvS968ADwFPeM8j3WdgGzAhaVvZfrbL/obk+OZ9AFiR8PxW4NZy11VgnxqTwr0VOMN7fAbQ6j2+H1iY3A5YCNyfsL1fu8H8D/glcGWl9BkYAbxE/B7Ee4Aab3vfzzXx+yZ8wHtc47Wz5J/1xHaD8R/xO7b9BrgMeMLrQ9T7nCrcy/azHbZpmVQ3655YplqKpd459zaA9/U0b3u6vofyPfH+9J5JfCQb6T570xOvALuAp4iPQN91znV7TRLr73ezeaD3ZvOh6jNwN/BV4IT3fDzR77MDfm1mL3r3i4Yy/myHbclfXzfijqh0fQ/de2Jmo4DHgS855w5muLFCJPrsnOsBLjSzMcAvgPelauZ9DX2fzexjwC7n3Itm1ty7OUXTyPTZc4lzbqeZnQY8ZWabM7Qtep/DNnL3c7PusHvHzM4A8L7u8ran63uo3hMzG0I82P+fc+7n3uZI97mXc+5dIEZ8jnWMxW8mD/3r7+ub9b/ZfJj6fAlwrZltAx4hPjVzN9HuM865nd7XXcR/ic+mjD/bYQt3PzfrDrvEm41fT3xeunf757yz7HOAA96feSuAq8xsrHcm/ipv26Bj8SH6T4BNzrnvJ+yKcp9P9UbsmNlw4ApgE7CS+M3kYWCfU91sfgmwwLuyZDIwBVhdml7kxjl3q3OuwTnXSPz/6G+dc39JhPtsZiPNbHTvY+I/k69Rzp/tcp+EyOOkxdXEr7J4E7it3PUU2JeHgbeB48R/Y99AfK7xN8Ab3tdxXlsD7vP6vR5oSjjO3wBt3r/Pl7tfGfr7QeJ/Yr4KvOL9uzrifZ4OvOz1+TXgdm/7OcSDqg34D2CYt73We97m7T8n4Vi3ee9FKzCv3H3z2f9mTl4tE9k+e31b5/3b0JtN5fzZ1idURUQiKGzTMiIi4oPCXUQkghTuIiIRpHAXEYkghbuISAQp3EVEIkjhLiISQQp3EZEI+v+0H5gJvN8OewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['acc'] = training_accuracy\n",
    "df.plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------NEURAL NETWORKS------------------------------------\n",
      "-----------------------Subtraction-------------------------------------\n",
      "Errors: 0  Correct :350\n",
      "Testing Accuracy: 100.0%\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "predictedTestLabelList = []\n",
    "\n",
    "\"\"\n",
    "for i,j in zip(processedTestingLabel,predictedTestLabel):\n",
    "    #predictedTestLabelList.append(decodeLabel(j))\n",
    "    \n",
    "    if np.argmax(i) == j:\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "print ('--------------------NEURAL NETWORKS------------------------------------')\n",
    "print ('-----------------------Subtraction-------------------------------------')\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: \" + str(right/(right+wrong)*100) +\"%\")\n",
    "print('-------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "------------------------THE END-----------------------------------------\n",
      "-------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------------------')\n",
    "print('-------------------------------------------------------------------------')\n",
    "print ('------------------------THE END-----------------------------------------')\n",
    "print('-------------------------------------------------------------------------')\n",
    "print('-------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
